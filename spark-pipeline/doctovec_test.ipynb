{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/hadoop/spark-2.2.2-bin-hadoop2.7')\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "conf = SparkConf().setMaster(\"spark://master:7077\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlcontext = SQLContext(sc)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count=sqlcontext.read.parquet('hdfs://master:9000/user/hadoop/token_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "\n",
    "#token_count_sum=token_count.groupBy('token').sum('count')\n",
    "#token_count_sum.show(5)\n",
    "\n",
    "#print('time :', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|title_id|              tokens|\n",
      "+--------+--------------------+\n",
      "|  112931|[정주행, 하시는, 분들, 추천...|\n",
      "|  112931|[만, 렙이, 레이, 드, 못,...|\n",
      "|  112931|[아랑소드, 용사, 시절, 에피...|\n",
      "|  112931|[본격, 드래곤, 이, 자기, ...|\n",
      "|  112931|[본격, 세계, 를구한, 드래곤...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2586400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "token_df = sqlcontext.read.parquet(\"hdfs://master:9000/user/hadoop/all_hangul_tokens\")\n",
    "token_df.select('title_id','tokens').show(5) #example\\\n",
    "token_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+--------------------+-------------------+\n",
      "|title_id|episode_num|comment_num|              tokens|    registered_time|\n",
      "+--------+-----------+-----------+--------------------+-------------------+\n",
      "|  112931|          0|          0|[정주행, 하시는, 분들, 추천...|2013-05-22 01:07:04|\n",
      "|  112931|          0|          1|[만, 렙이, 레이, 드, 못,...|2013-04-15 01:47:32|\n",
      "|  112931|          0|          2|[아랑소드, 용사, 시절, 에피...|2013-05-02 08:37:28|\n",
      "|  112931|          0|          3|[본격, 드래곤, 이, 자기, ...|2013-04-19 06:56:25|\n",
      "|  112931|          0|          4|[본격, 세계, 를구한, 드래곤...|2013-04-26 07:52:28|\n",
      "+--------+-----------+-----------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "split_udf=F.udf(lambda s: s[1:-1])\n",
    "temp = token_df.select('title_id','tokens').withColumn('split', F.split(split_udf(token_df.tokens),', ')).select('title_id','split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|title_id|              tokens|               split|\n",
      "+--------+--------------------+--------------------+\n",
      "|  112931|[정주행, 하시는, 분들, 추천...|[정주행, 하시는, 분들, 추천...|\n",
      "|  112931|[만, 렙이, 레이, 드, 못,...|[만, 렙이, 레이, 드, 못,...|\n",
      "|  112931|[아랑소드, 용사, 시절, 에피...|[아랑소드, 용사, 시절, 에피...|\n",
      "|  112931|[본격, 드래곤, 이, 자기, ...|[본격, 드래곤, 이, 자기, ...|\n",
      "|  112931|[본격, 세계, 를구한, 드래곤...|[본격, 세계, 를구한, 드래곤...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#split_udf=F.udf(lambda x : ''.join((x[1:-1]).replace(\" \", \"\")).split(\",\"))\n",
    "#token_df=token_df.select('title_id','tokens').withColumn('split', split_udf(token_df.tokens))\n",
    "\n",
    "split_udf=F.udf(lambda s: s[1:-1])\n",
    "token_df=token_df.select('title_id','tokens').withColumn('split', F.split(split_udf(token_df.tokens),', ') )\n",
    "\n",
    "token_df.show(5)\n",
    "#mapped_df=token_df.select(token_df.title_id, F.explode(token_df.split).alias('token'), F.lit(1)).filter(F.length('token')>1)\n",
    "#mapped_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title_id: integer (nullable = true)\n",
      " |-- tokens: string (nullable = true)\n",
      " |-- split: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------+--------------------+\n",
      "|title_id|               split|\n",
      "+--------+--------------------+\n",
      "|  112931|[정주행, 하시는, 분들, 추천...|\n",
      "|  112931|[만, 렙이, 레이, 드, 못,...|\n",
      "|  112931|[아랑소드, 용사, 시절, 에피...|\n",
      "|  112931|[본격, 드래곤, 이, 자기, ...|\n",
      "|  112931|[본격, 세계, 를구한, 드래곤...|\n",
      "|  112931|         [이거, 명작, 임]|\n",
      "|  112931|[본격, 나런급의, 꿈도, 희망...|\n",
      "|  112931|[이, 만화, 가, 그렇게, 멘...|\n",
      "|  112931|[만렙, 찍고, 나면, 초급마을...|\n",
      "|  112931|[엄청, 난, 명작이, 지만, ...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "temp_tokendf = token_df.select(\"split\").take(5)\n",
    "\n",
    "split_df = token_df.select(\"title_id\", \"split\")\n",
    "split_df = split_df.limit(10000)\n",
    "\n",
    "\n",
    "#a = token_df.rdd.map(lambda x: x.split(\",\")).toDF()\n",
    "\n",
    "\n",
    "\n",
    "#print(type(temp_tokendf))\n",
    "#print(temp_tokendf)\n",
    "\n",
    "#for i in temp_tokendf:\n",
    "#    print(type(i))\n",
    "#    print(''.join((i['tokens'][1:-1]).replace(\" \", \"\")).split(\",\"))\n",
    "\n",
    "    \n",
    "#temp_tokendf = token_df.select('tokens').apply(lambda x : ''.join((x[1:-1]).replace(\" \", \"\")).split(\",\")).toDF()\n",
    "\n",
    "\n",
    "#token_df.select(col(\"tokens\"))\n",
    "\n",
    "token_df.printSchema()\n",
    "\n",
    "#token_df.select(\"split\").collect()\n",
    "split_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 14.921932220458984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2Vec_4d36a8c4993747f02229"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "#embedding_model = Word2Vec(temp_tokendf, size=100, window = 2, min_count=1, workers=4, iter=100, sg=1)\n",
    "\n",
    "\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"split\", outputCol=\"result\")\n",
    "\n",
    "\n",
    "#word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(split_df)\n",
    "\n",
    "print('time :', time.time() - start)\n",
    "\n",
    "model\n",
    "#result = model.transform(documentDF)\n",
    "#for row in result.collect():\n",
    "#    text, vector = row\n",
    "#    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  word|              vector|\n",
      "+------+--------------------+\n",
      "|   선우가|[0.00404577329754...|\n",
      "|    받음|[0.00537162646651...|\n",
      "|   훈훈한|[0.00361029850319...|\n",
      "|    쿤씨|[-0.0059975041076...|\n",
      "|  안이뤄지|[0.00214941264130...|\n",
      "|    근헤|[0.00117842713370...|\n",
      "|   좋았던|[0.01105631049722...|\n",
      "|보냈었는데요|[-0.0078351572155...|\n",
      "| 작성해보았|[0.01037278026342...|\n",
      "|    안한|[0.00444051902741...|\n",
      "|   주연이|[-0.0035231341607...|\n",
      "|    사진|[-0.0106863882392...|\n",
      "|  아그네스|[0.00708514777943...|\n",
      "|   톱질해|[2.21347960177809...|\n",
      "| 치욕스러운|[-0.0021920048166...|\n",
      "|    암시|[0.03119258582592...|\n",
      "|     스|[-0.0781821087002...|\n",
      "|들볶고싶구나|[-0.0021493446547...|\n",
      "|    마이|[0.00452623143792...|\n",
      "|   속세로|[-0.0052055893465...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordVectorsDF = model.getVectors()\n",
    "wordVectorsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|word|        similarity|\n",
      "+----+------------------+\n",
      "|  물어|0.6366880536079407|\n",
      "| 해주는|0.6241371631622314|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms('받음',2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='선우가', vector=DenseVector([0.004, -0.0012, -0.0021, 0.0047, -0.0043, -0.0035, 0.0027, 0.0034, 0.0012, 0.0021, 0.0036, -0.0051, -0.0045, 0.0048, 0.0009, -0.0012, 0.0046, -0.0001, -0.0016, 0.0031, -0.0041, -0.0025, 0.0016, 0.0013, 0.0006, -0.006, -0.002, -0.0006, 0.0006, 0.0013, -0.0017, -0.0051, 0.0058, 0.0029, -0.003, -0.0019, -0.0008, -0.0008, -0.0031, -0.0061, -0.0023, -0.0047, 0.0006, 0.0055, 0.0009, -0.0015, 0.0035, -0.0014, 0.0032, 0.0017, 0.0016, 0.0021, 0.0033, 0.0031, -0.0038, -0.0004, 0.0046, -0.0008, -0.0025, 0.0049, -0.0026, 0.0036, -0.0037, -0.0007, -0.0004, -0.0053, 0.0019, 0.0052, -0.0003, -0.0053, -0.0023, -0.0015, -0.0001, 0.0037, 0.0079, 0.0043, -0.0039, 0.0004, -0.0039, -0.006, -0.0002, 0.001, 0.0002, 0.0002, -0.0003, 0.0074, -0.0038, 0.0035, -0.0005, 0.0013, -0.0036, -0.0057, -0.005, 0.0079, -0.0091, 0.0002, -0.0017, 0.0006, -0.0043, -0.0042]))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVectorsDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vecs_df = model.transform(split_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|title_id|               split|              result|\n",
      "+--------+--------------------+--------------------+\n",
      "|  112931|[정주행, 하시는, 분들, 추천...|[0.01058779986245...|\n",
      "|  112931|[만, 렙이, 레이, 드, 못,...|[0.01010942008320...|\n",
      "|  112931|[아랑소드, 용사, 시절, 에피...|[-0.0267542001092...|\n",
      "|  112931|[본격, 드래곤, 이, 자기, ...|[0.00634275979828...|\n",
      "|  112931|[본격, 세계, 를구한, 드래곤...|[-0.0010553386739...|\n",
      "|  112931|         [이거, 명작, 임]|[0.03141902418186...|\n",
      "|  112931|[본격, 나런급의, 꿈도, 희망...|[0.01205064291467...|\n",
      "|  112931|[이, 만화, 가, 그렇게, 멘...|[0.02627593645145...|\n",
      "|  112931|[만렙, 찍고, 나면, 초급마을...|[0.00642088868854...|\n",
      "|  112931|[엄청, 난, 명작이, 지만, ...|[0.01919694915515...|\n",
      "|  112931|[어떤, 용사, 의, 초건빵포,...|[-1.3217547287543...|\n",
      "|  112931|         [멘붕, 의, 시작]|[0.03369739154974...|\n",
      "|  112931|[이거, 진짜, 명작, 임, 방...|[0.01971330972238...|\n",
      "|  112931| [이거, 명작, 임, 내가, 봤음]|[0.01982798180542...|\n",
      "|  112931|   [드레곤, 세상을, 구, 했어]|[0.01291798986494...|\n",
      "|  112931|[유료화, 하는, 게, 작가님,...|[0.01367753587276...|\n",
      "|  112931|[나한테, 네이버, 판타지, 투...|[0.00310096746155...|\n",
      "|  112931|[방금, 처음, 으로, 정주행,...|[0.00565246856140...|\n",
      "|  112931|[결말, 가지고, 뭐라, 하는,...|[0.00708888046331...|\n",
      "|  112931|         [년, 보시는, 분]|[0.03076562533775...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc2vecs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_train_df, w2v_test_df = doc2vecs_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "si = StringIndexer(inputCol=\"title_id\", outputCol=\"label\")\n",
    "rf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"result\")\n",
    "\n",
    "rf_classifier_pipeline = Pipeline(stages=[si,rf_classifier])\n",
    "rf_predictions = rf_classifier_pipeline.fit(w2v_train_df).transform(w2v_test_df)\n",
    "\n",
    "rf_model_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.692587\n"
     ]
    }
   ],
   "source": [
    "accuracy = rf_model_evaluator.evaluate(rf_predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
